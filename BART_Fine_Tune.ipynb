{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFVRTmkPXqG8"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets evaluate accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        ")"
      ],
      "metadata": {
        "id": "AIUNXvGAXuFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"cnn_dailymail\"\n",
        "dataset_config = \"3.0.0\"\n",
        "\n",
        "raw_datasets = load_dataset(dataset_name, dataset_config)\n",
        "print(raw_datasets)\n",
        "\n",
        "# Columns: \"article\" (input), \"highlights\" (summary)\n",
        "text_column = \"article\"\n",
        "summary_column = \"highlights\"\n",
        "\n",
        "# ⚡ To keep training fast in Colab, we take a small subset\n",
        "train_subset = 2000\n",
        "eval_subset = 1000\n",
        "\n",
        "raw_datasets[\"train\"] = raw_datasets[\"train\"].select(range(min(train_subset, len(raw_datasets[\"train\"]))))\n",
        "raw_datasets[\"validation\"] = raw_datasets[\"validation\"].select(range(min(eval_subset, len(raw_datasets[\"validation\"]))))"
      ],
      "metadata": {
        "id": "zTx3PxOTXuHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "HBuRg78BXuKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_source_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples[text_column]\n",
        "    targets = examples[summary_column]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        inputs, max_length=max_source_length, padding=\"max_length\", truncation=True\n",
        "    )\n",
        "    labels = tokenizer(\n",
        "        targets, max_length=max_target_length, padding=\"max_length\", truncation=True\n",
        "    )\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        "    desc=\"Tokenizing\",\n",
        ")"
      ],
      "metadata": {
        "id": "XDOtCn2fXuMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    return {k: round(v * 100, 2) for k, v in result.items()}\n"
      ],
      "metadata": {
        "id": "8pzlpt5CXuOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./bart-cnn-finetune\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    logging_steps=50,\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,   # ⚡ Change to 2–3 if GPU allows\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=128,\n",
        "    generation_num_beams=4,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "NRWeHOu1XuQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "7YCuWu9oXuSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "OhvAm3szXuUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"./bart-cnn-finetune\")\n",
        "tokenizer.save_pretrained(\"./bart-cnn-finetune\")"
      ],
      "metadata": {
        "id": "iYcZL0r7XuWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = raw_datasets[\"validation\"][0][\"article\"]\n",
        "inputs = tokenizer([sample_text], max_length=512, truncation=True, return_tensors=\"pt\").to(model.device)\n",
        "summary_ids = model.generate(**inputs, num_beams=4, max_length=128)\n",
        "print(\"ARTICLE:\", sample_text[:400], \"...\")\n",
        "print(\"SUMMARY:\", tokenizer.decode(summary_ids[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "POT5kUlKXuYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ahoi2_AaXucT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}